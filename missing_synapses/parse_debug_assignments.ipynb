{
 "metadata": {
  "name": "",
  "signature": "sha256:94d98951249dbaf7d50150b769127fb34194ebb427f69a446782e052cb97b6b0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Script to Convert Focused Debug Output To Merge Coordinates\n",
      "\n",
      "### Pipeline Before Running This Script\n",
      "* Received from Ting a list of bodies that were unexamined and candidate bodies that connect to them\n",
      "* Extracted a subset of these candidate edges (limited to those that would not result in future tracing)\n",
      "* Proofreaders performed focused debug protocol on these candidate edges producing a sessionpriority json files encoding 0/1 decisions\n",
      "* Proofreaders also generated bookmarks for locations that will need to be manually examined\n",
      "* Coordinate system corresponds to original 13 layer stitched volume\n",
      "\n",
      "### What this script does\n",
      "* Takes new coordinate system\n",
      "* Takes bookmark glob list and outputs one bookmark file in new coordinate system\n",
      "* Takes session json glob list and outputs one bookmark file in new coordinate system showing merge points\n",
      "* Uses Ting's original unexamined list to determine the number of recovered annotations and the optimal body locations for merge"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "All input/output files are located at: /groups/flyem/data/medulla-FIB-Z1211-25-production/align2/base_stacks/shinya1-13_20140516"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prefix=\"/groups/flyem/data/medulla-FIB-Z1211-25-production/align2/base_stacks/shinya1-13_20140516\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_focused_output(xshift, yshift, session_files, bookmark_files, original_orphan_file, result_directory):\n",
      "    import glob\n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    # create bookmark output directory and file\n",
      "    bookmark_list = glob.glob(bookmark_files)\n",
      "    try:\n",
      "        os.makedirs(result_directory)\n",
      "    except OSError:\n",
      "        pass\n",
      "    bookmark_output = open(result_directory + \"/bookmarks.json\", 'w')\n",
      "    bookmarks = []\n",
      "    \n",
      "    num_bookmarks = 0\n",
      "    \n",
      "    # read and accumulate bookmarks\n",
      "    for bookmark_file in bookmark_list:\n",
      "        data = json.load(open(bookmark_file))\n",
      "        for bookmark in data[\"data\"]:\n",
      "            bookmark[\"location\"][0] += xshift\n",
      "            bookmark[\"location\"][1] += yshift\n",
      "            bookmarks.append(bookmark)\n",
      "            num_bookmarks += 1\n",
      "    \n",
      "    # write bookmark file\n",
      "    bookmark_data = {}\n",
      "    bookmark_data[\"data\"] = bookmarks\n",
      "    bookmark_data[\"metadata\"] = {\"file version\": 1, \"description\": \"bookmarks\"}\n",
      "    bookmark_output.write(json.dumps(bookmark_data, indent=4))\n",
      "    bookmark_output.close()\n",
      "\n",
      "    # retrieve all body locations\n",
      "    orphan_data = json.load(open(original_orphan_file))\n",
      "    \n",
      "    body_locations = {}\n",
      "    unexamined_bodies = set()\n",
      "    num_synapses = {}\n",
      "    for body in orphan_data[\"Bodies\"]:\n",
      "        body_locations[body[\"id\"]] = body[\"marker\"]\n",
      "        if not body[\"cross_substack\"] and body[\"size\"] < 100000:\n",
      "            unexamined_bodies.add(body[\"id\"])\n",
      "            num_synapses[body[\"id\"]] = body[\"num_synapses\"]\n",
      "    \n",
      "    # create accumulation file\n",
      "    merge_output = open(result_directory + \"/pointstolink.json\", 'w')\n",
      "    \n",
      "    num_merges = 0\n",
      "    num_total = 0\n",
      "    \n",
      "    bodylink_list = []\n",
      "    \n",
      "    # accumulate all merges\n",
      "    total_synapses_fixed = 0\n",
      "    session_list = glob.glob(session_files)\n",
      "    for session_file in session_list:\n",
      "        data = json.load(open(session_file))\n",
      "        for edge in data[\"edge_list\"]:\n",
      "            num_total += 1\n",
      "            if edge[\"weight\"] == 0.0:\n",
      "                num_merges += 1\n",
      "                point_pair = []\n",
      "                \n",
      "                # make sure unexamined node is second\n",
      "                if edge[\"node1\"] in unexamined_bodies:\n",
      "                    point_pair.append(body_locations[edge[\"node2\"]])\n",
      "                    point_pair.append(body_locations[edge[\"node1\"]])\n",
      "                    total_synapses_fixed += num_synapses[edge[\"node1\"]]\n",
      "                else:\n",
      "                    if edge[\"node2\"] not in unexamined_bodies:\n",
      "                        print \"Node must not be examined to go first\"\n",
      "                    point_pair.append(body_locations[edge[\"node1\"]])\n",
      "                    point_pair.append(body_locations[edge[\"node2\"]])\n",
      "                    total_synapses_fixed += num_synapses[edge[\"node2\"]]\n",
      "                    \n",
      "                # shift the coordinates for the big stack\n",
      "                point_pair[0][0] += xshift\n",
      "                point_pair[0][1] += yshift\n",
      "                point_pair[1][0] += xshift\n",
      "                point_pair[1][1] += yshift\n",
      "                \n",
      "                bodylink_list.append(point_pair)\n",
      "    \n",
      "    bodylink_data = {}\n",
      "    bodylink_data[\"data\"] = bodylink_list\n",
      "    bodylink_data[\"metadata\"] = {\"file version\": 1, \"description\": \"link-save export\"}\n",
      "    merge_output.write(json.dumps(bodylink_data, indent=4))\n",
      "    merge_output.close()\n",
      "    \n",
      "    print \"Number of bookmarks: \", num_bookmarks\n",
      "    print \"Number of total edges: \", num_total\n",
      "    print \"Number of merged edges: \", num_merges\n",
      "    print \"Number of synapse annotations added to examined volume: \", total_synapses_fixed\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Run command with input files from the default prefix,  output to focused-debug-results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# call function with x offset 1878 and y offset 2364\n",
      "process_focused_output(1878, 2364, prefix + \"/focused-debug-output/sessionpriority_debug_orphan/*.json\",\n",
      "                       prefix + \"/focused-debug-output/bookmarks_debug_orphan/*.json\",\n",
      "                       prefix + \"/focused-debug/input/input.json\", prefix + \"/focused-debug-results\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of bookmarks:  227\n",
        "Number of total edges:  8888\n",
        "Number of merged edges:  4644\n",
        "Number of synapse annotations added to examined volume:  7157\n"
       ]
      }
     ],
     "prompt_number": 9
    }
   ],
   "metadata": {}
  }
 ]
}